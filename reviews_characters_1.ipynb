{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is to answer the subquestion (1) Are readers' book reviews about Science Fiction on the Goodreads website generally more positive or more negative? (2) Is there a correlation between multiple variables of book reviews, such as ratings, the review length, the book review support, and so on?"
      ],
      "metadata": {
        "id": "3PC8nR9jjAZG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ungzOAVmNxMp"
      },
      "source": [
        "## **1.Clean data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is necessary to clean the book review text because it contains a lot of issues. For example, the text includes URLs, emojis, and spelling errors which will affect the result of the model."
      ],
      "metadata": {
        "id": "gtnikeI_RWAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: make sure you have installed the necessary libraries, such as langdetect and hunspell, before running the code. You can use pip, the Python package installer, to install these libraries."
      ],
      "metadata": {
        "id": "nI3P7ChxT2uD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC807xB3D72d",
        "outputId": "09d514e1-6fd7-4818-d71a-b71bfafd5f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kciqDd_3dVqz",
        "outputId": "9a73f436-20eb-449a-9801-2f29a20f2d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=c7f989647d1b6477dcba6f608f4c8a18f2b20a54b04e55496b2d6821b703910b\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Wcucf997iK",
        "outputId": "85ad392f-c39f-4e12-bcf6-ba010cc8c1f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ubuntu.com (91.189.91.8\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to security.ubuntu.com (91.189.91.8\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r                                                                                                    \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [47.6 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,084 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,129 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,858 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,377 kB]\n",
            "Fetched 6,907 kB in 2s (3,522 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dictionaries-common hunspell-en-us libhunspell-1.7-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  wordlist openoffice.org-hunspell | openoffice.org-core\n",
            "The following NEW packages will be installed:\n",
            "  dictionaries-common hunspell hunspell-en-us libhunspell-1.7-0 libhunspell-dev libtext-iconv-perl\n",
            "0 upgraded, 6 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 964 kB of archives.\n",
            "After this operation, 3,310 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 hunspell amd64 1.7.0-4build1 [67.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-dev amd64 1.7.0-4build1 [241 kB]\n",
            "Fetched 964 kB in 1s (1,241 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../1-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../2-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../3-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package hunspell.\n",
            "Preparing to unpack .../4-hunspell_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking hunspell (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libhunspell-dev:amd64.\n",
            "Preparing to unpack .../5-libhunspell-dev_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-dev:amd64 (1.7.0-4build1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libhunspell-dev:amd64 (1.7.0-4build1) ...\n",
            "Setting up hunspell (1.7.0-4build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "Collecting hunspell\n",
            "  Downloading hunspell-0.5.5.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: hunspell\n",
            "  Building wheel for hunspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hunspell: filename=hunspell-0.5.5-cp310-cp310-linux_x86_64.whl size=66269 sha256=f1115ee58cfffa4286a08d4695fc23ce4634ae4441bd1f817048fb5d4a1b1816\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/f3/bd/bdce223532ee8aa5345e14e0d6e7ba06cbbaff8767cefe1ec8\n",
            "Successfully built hunspell\n",
            "Installing collected packages: hunspell\n",
            "Successfully installed hunspell-0.5.5\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y hunspell libhunspell-dev\n",
        "!pip install hunspell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGpvVjUtNwEt"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "import hunspell\n",
        "import pandas as pd\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc-9qXt_DQo7",
        "outputId": "a5a3a0d9-9a9d-460e-90a2-17ebb35e4c9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgP1ElACEABs",
        "outputId": "cf91bbb0-2bb7-4bac-fba2-03d103fa9083"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZD7hhqGUnCZ"
      },
      "source": [
        "### 1.1 Delete non-English reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the Goodreads website is an international reading platform where readers come from all over the world and the language of book reviews varies, only the language English was selected in this study."
      ],
      "metadata": {
        "id": "DwPzvQBLR6Bl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9v7mPCkUoxb",
        "outputId": "f4da6924-80f2-4036-a940-83bcaa24cbca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing completed. English-only reviews have been saved in: /content/drive/MyDrive/page_5_reviews_english_only\n"
          ]
        }
      ],
      "source": [
        "# Define a function named detect_language that takes a parameter 'text'.\n",
        "def detect_language(text):\n",
        "    try: # Begin a try block to handle potential exceptions.\n",
        "        # Check if the input 'text' is a string.\n",
        "        if isinstance(text, str):\n",
        "            return detect(text)  # If 'text' is a string, detect its language using the detect function and return the result.\n",
        "        else: # If 'text' is not a string:\n",
        "            return \"unknown\" # Return \"unknown\" as the language cannot be detected.\n",
        "    except LangDetectException:  # If a LangDetectException is raised during detection:\n",
        "        return \"unknown\"  # Return \"unknown\" as the language detection failed.\n",
        "\n",
        "# Set the input folder path where the CSV files with reviews are stored.\n",
        "input_folder_path = \"/content/drive/MyDrive/page_5_reviews\"\n",
        "# Set the output folder path where the filtered English-only reviews will be saved.\n",
        "output_folder_path = \"/content/drive/MyDrive/page_5_reviews_english_only\"\n",
        "\n",
        "# Loop through each file in the input folder.\n",
        "for filename in os.listdir(input_folder_path):\n",
        "    if filename.endswith(\".csv\"):   # Check if the file is a CSV file.\n",
        "        # Construct the full file path of the input CSV file.\n",
        "        file_path = os.path.join(input_folder_path, filename)\n",
        "\n",
        "        # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "        data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "        # Apply the detect_language function to the 'Content' column and create a new 'Language' column with the results.\n",
        "        data['Language'] = data['Content'].apply(detect_language)\n",
        "\n",
        "        # Filter the DataFrame to include only rows where the 'Language' column is 'en' (English).\n",
        "        data = data[data['Language'] == 'en']\n",
        "\n",
        "        # Construct the full file path for the output CSV file.\n",
        "        output_file_path = os.path.join(output_folder_path, filename)\n",
        "        # Save the filtered DataFrame to the output path, without including the index.\n",
        "        data.to_csv(output_file_path, index=False)\n",
        "# Print a message indicating the processing is completed and the location of the saved English-only reviews.\n",
        "print(\"Processing completed. English-only reviews have been saved in:\", output_folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3100l3Y0UVAt"
      },
      "source": [
        "### 1.2 Removing rows containing null values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset, there are missing values in the columns “Review Count”, “Followers”, “Rating”, and “Likes”. Since our research question is based on the correlation between these variables, it is crucial to ensure data integrity. Therefore, we first use the “isnull” method to detect missing values in these four columns, and then employ the “any” method to filter out rows where any of these columns contain missing values."
      ],
      "metadata": {
        "id": "ERFKNsM6_SPU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8kDSQWHQM6B",
        "outputId": "271c039d-6785-4bff-a9c4-f92748bf8154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been successfully filtered for all files!\n"
          ]
        }
      ],
      "source": [
        "# Set the folder path where the filtered English-only review CSV files are stored.\n",
        "folder_path = \"/content/drive/MyDrive/page_5_reviews_english_only\"\n",
        "\n",
        "# Create a list of file paths for all CSV files in the folder.\n",
        "csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
        "\n",
        "# Specify the columns that need to be checked for null values.\n",
        "columns_to_check = [\"Review Count\", \"Followers\", \"Rating\", \"Likes\"]\n",
        "\n",
        "# Loop through each CSV file in the list of file paths.\n",
        "for file_path in csv_files:\n",
        "    # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "    data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "    # Check if the 'Tags' column exists in the DataFrame.\n",
        "    if 'Tags' in data.columns:\n",
        "        data.drop(columns=['Tags'], inplace=True)  # If the 'Tags' column exists, drop it from the DataFrame.\n",
        "\n",
        "    # Identify rows that have any null values in the specified columns.\n",
        "    rows_with_nulls = data[data[columns_to_check].isnull().any(axis=1)]\n",
        "\n",
        "    # If there are rows with null values in the specified columns:\n",
        "    if not rows_with_nulls.empty:\n",
        "        data = data.dropna(subset=columns_to_check)  # Drop rows with null values in the specified columns from the DataFrame.\n",
        "    else:\n",
        "        pass  # If there are no rows with null values, do nothing.\n",
        "\n",
        "    # Save the filtered DataFrame back to the same CSV file path, without including the index.\n",
        "    data.to_csv(file_path, index=False)\n",
        "\n",
        "# Print a message indicating the data filtering process is completed for all files.\n",
        "print(\"Data has been successfully filtered for all files!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qENSXzm1b27i"
      },
      "source": [
        "### 1.3 Cleaning up the book reviews text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Book reviews contain a lot of issues. For example, the text includes URLs, emojis, and spelling errors which will affect the result of the model. So, what we need to do is to fix these problems."
      ],
      "metadata": {
        "id": "CxRor_59F4sE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQZpGmKoVOxx",
        "outputId": "c500f41b-0b95-4393-e9a7-ef7499ee5e0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been successfully written to the file！\n"
          ]
        }
      ],
      "source": [
        "# Set the folder path where the filtered English-only review CSV files are stored.\n",
        "folder_path = \"/content/drive/MyDrive/page_5_reviews_english_only/\"\n",
        "\n",
        "# Create a list of filenames for all CSV files in the folder.\n",
        "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Loop through each CSV file in the list of filenames.\n",
        "for file in csv_files:\n",
        "    # Construct the full file path of the input CSV file.\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "\n",
        "    # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "    data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "    # Define a function to remove URLs from a given text.\n",
        "    def remove_urls(text):\n",
        "        # Compile a regex pattern to match URLs starting with http, https, or www.\n",
        "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        # Substitute matched URLs with an empty string to remove them from the text.\n",
        "        return url_pattern.sub(r'', text)\n",
        "\n",
        "    # Apply the remove_urls function to the 'Content' column and create a new column 'remove_urls_reviews'.\n",
        "    data['remove_urls_reviews'] = data['Content'].apply(remove_urls)\n",
        "\n",
        "    # Define a function to remove non-alphabetical characters from a given text.\n",
        "    def remove_non_text(text):\n",
        "        # Create a regex pattern to match any character that is not a letter or whitespace.\n",
        "        processed_text = r'[^a-zA-Z\\s]'\n",
        "        # Substitute matched characters with a space to remove non-alphabetical characters.\n",
        "        processed_text = re.sub(processed_text, ' ', text)\n",
        "        return processed_text\n",
        "\n",
        "    # Apply the remove_non_text function to the 'remove_urls_reviews' column and create a new column 'processed_reviews'.\n",
        "    data['processed_reviews'] = data['remove_urls_reviews'].apply(remove_non_text)\n",
        "\n",
        "    # Initialize the HunSpell spell checker with the English dictionary and affix files.\n",
        "    spell_checker = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n",
        "\n",
        "    # Define a function to correct spelling errors in a given text.\n",
        "    def correct_spelling(text):\n",
        "        # Split the text into individual words.\n",
        "        words = text.split()\n",
        "        # Initialize an empty list to store the corrected words.\n",
        "        corrected_words = []\n",
        "        # Loop through each word in the list of words.\n",
        "        for word in words:\n",
        "            # Check if the word is spelled correctly using the spell checker.\n",
        "            if not spell_checker.spell(word):\n",
        "                # If the word is misspelled, get suggestions for the correct spelling.\n",
        "                suggestions = spell_checker.suggest(word)\n",
        "                # If there are suggestions, take the first suggestion as the corrected word.\n",
        "                if suggestions:\n",
        "                    corrected_word = suggestions[0]\n",
        "                    corrected_words.append(corrected_word)  # Append the corrected word to the list of corrected words.\n",
        "                else:\n",
        "                    # If no suggestions are available, keep the original word.\n",
        "                    corrected_words.append(word)\n",
        "            else:\n",
        "                corrected_words.append(word) # If the word is spelled correctly, keep it as is.\n",
        "        # Join the list of corrected words back into a single string and return it.\n",
        "        return ' '.join(corrected_words)\n",
        "\n",
        "    # Apply the correct_spelling function to the 'processed_reviews' column and create a new column 'corrected_reviews'.\n",
        "    data['corrected_reviews'] = data['processed_reviews'].apply(correct_spelling)\n",
        "\n",
        "    # Initialize the WordNet lemmatizer for reducing words to their base form.\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Define a function to lemmatize each word in the given text.\n",
        "    def lemmatize_text(text):\n",
        "        # Split the text into words, lemmatize each word, and join them back into a single string.\n",
        "        lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "        return lemmatized_text\n",
        "\n",
        "    # Apply the lemmatize_text function to the 'corrected_reviews' column and create a new column 'lemmatized_reviews'.\n",
        "    data['lemmatized_reviews'] = data['corrected_reviews'].apply(lemmatize_text)\n",
        "\n",
        "    # Convert all text in the 'lemmatized_reviews' column to lowercase.\n",
        "    data['lemmatized_reviews'] = data['lemmatized_reviews'].str.lower()\n",
        "\n",
        "    # Import the set of English stop words from the NLTK library.\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Define a function to remove stop words from the given text.\n",
        "    def remove_stop_words(text):\n",
        "        # Split the text into words, filter out the stop words, and join the remaining words back into a single string.\n",
        "        filtered_text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "        return filtered_text\n",
        "\n",
        "    # Apply the remove_stop_words function to the 'lemmatized_reviews' column and create a new column 'stop_words_removed'.\n",
        "    data['stop_words_removed'] = data['lemmatized_reviews'].apply(remove_stop_words)\n",
        "    # Rename the 'stop_words_removed' column to 'cleaned_reviews'.\n",
        "    data['cleaned_reviews'] = data['lemmatized_reviews'].apply(remove_stop_words)\n",
        "\n",
        "    # Drop intermediate columns that are no longer needed from the DataFrame.\n",
        "    data = data.drop(columns=['remove_urls_reviews', 'processed_reviews','corrected_reviews','lemmatized_reviews','stop_words_removed'])\n",
        "\n",
        "    # Save the cleaned DataFrame back to the same CSV file path, without including the index.\n",
        "    data.to_csv(file_path, index=False)\n",
        "\n",
        "# Print a message indicating the data has been successfully written to the file.\n",
        "print(\"Data has been successfully written to the file！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fZMPsItwRN1"
      },
      "source": [
        "## 2.calculate variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this study, we specifically analyze the correlation between the following variables:\n",
        "\n",
        "(1)  the review length; (2)  the number of reviews that a reviewer has written;\n",
        "(3)  the number of followers of the reviewer; (4)  the days passed since the book review have been posted; (5)  the sentiment score of the book review; (6)  the rating of book; (7)  the book review support\n",
        "\n",
        "\n",
        "All variables need to be calculated to get except (2) the number of reviews that a reviewer has written, and (3) the number of followers of the reviewer, which can be obtained directly."
      ],
      "metadata": {
        "id": "h8E6OaxDmU6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Calculate the number of days since a given date"
      ],
      "metadata": {
        "id": "wgt0wgpiQQgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du2L0Kgq2JLo"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeXQKW_Z_77G",
        "outputId": "a27298a9-6e19-43e6-eb7b-84b71719f762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been successfully converted！\n"
          ]
        }
      ],
      "source": [
        "# Set the folder path where the filtered English-only review CSV files are stored.\n",
        "folder_path = \"/content/drive/MyDrive/page_5_reviews_english_only\"\n",
        "\n",
        "# Define a function to convert a date string into a datetime object.\n",
        "def convert_to_date(date_str):\n",
        "    # List of possible date formats to try.\n",
        "    formats = [\"%B %d, %Y\", \"%d-%b-%y\"]\n",
        "    # Loop through each date format.\n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            # Attempt to parse the date string using the current format.\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except ValueError:\n",
        "            # If parsing fails, continue to the next format.\n",
        "            continue\n",
        "    # Raise an error if the date string doesn't match any of the formats.\n",
        "    raise ValueError(f\"Date format does not match: {date_str}\")\n",
        "\n",
        "# Define a function to calculate the number of days since a given date.\n",
        "def calculate_days_since(date_str, reference_date):\n",
        "    # Convert the date string to a datetime object.\n",
        "    date = convert_to_date(date_str)\n",
        "    # Calculate the difference in days between the reference date and the given date.\n",
        "    return (reference_date - date).days\n",
        "\n",
        "# Set the reference date for the calculation.\n",
        "reference_date = datetime(2024, 5, 28)\n",
        "\n",
        "# Loop through each file in the folder.\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".csv\"): # Check if the file is a CSV file.\n",
        "        # Construct the full file path of the input CSV file.\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "        data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "        # Apply the calculate_days_since function to the 'Date' column and create a new column 'Number_of_Days'.\n",
        "        data['Number_of_Days'] = data['Date'].apply(lambda x: calculate_days_since(x, reference_date))\n",
        "\n",
        "        # Save the updated DataFrame back to the same CSV file path, without including the index.\n",
        "        data.to_csv(file_path, index=False)\n",
        "\n",
        "# Print a message indicating the data conversion process is completed for all files.\n",
        "print(\"Data has been successfully converted！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLWGsTbI5iKb"
      },
      "source": [
        "### 2.2 Calculate the number of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktGILGmjJjoz"
      },
      "outputs": [],
      "source": [
        "# import required library\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQLBwEOYJaCC",
        "outputId": "77f5949f-ffb1-484e-fb44-bfb3d2e859f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been successfully flitered！\n"
          ]
        }
      ],
      "source": [
        "# Set the folder path where the filtered English-only review CSV files are stored.\n",
        "folder_path = \"/content/drive/MyDrive/page_5_reviews_english_only\"\n",
        "\n",
        "# Loop through each file in the folder.\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".csv\"): # Check if the file is a CSV file.\n",
        "        # Construct the full file path of the input CSV file.\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "        data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "        # Create a new column 'word_count' by applying a lambda function to each entry in the 'cleaned_reviews' column.\n",
        "        # Calculate the number of words if the entry is a string; otherwise, set to 0.\n",
        "        data['word_count'] = data['cleaned_reviews'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
        "\n",
        "        # Save the updated DataFrame back to the same CSV file path, without including the index.\n",
        "        data.to_csv(file_path, index=False)\n",
        "\n",
        "# Print a message indicating the data processing is completed for all files.\n",
        "print(\"Data has been successfully flitered！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB3bvIcHOFl3"
      },
      "source": [
        "### 2.3 Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We downloaded and used a model provided by Pianzola that was specially designed to categorize the sentiment of book reviews. The output contains the probability of positive, the probability of negative, the probability of neutral, the confidence of the classification and the result of the sentiment classification. For the convenience of the analysis, the probability of positive is used to represent the sentiment value of the book reviews for the subsequent correlation analysis."
      ],
      "metadata": {
        "id": "eZNLN_b1rX98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy4dwy_9gFJP",
        "outputId": "ffc3e3ad-0a83-4fa6-9c59-d2a2057d2b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.26.0)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.26.0\n",
            "    Uninstalling transformers-4.26.0:\n",
            "      Successfully uninstalled transformers-4.26.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "diHrD1A2Wps5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder containing the input CSV files\n",
        "folder_path = \"/content/drive/MyDrive/page_5_reviews_english_only/\"\n",
        "# Define the path to the folder where the output CSV files will be saved\n",
        "output_folder_path = \"/content/drive/MyDrive/page_5_reviews_positive_prob/\"\n",
        "\n",
        "# Create a list of all CSV file names in the input folder\n",
        "# This filters the files in the folder to include only those that end with '.csv'\n",
        "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Load a sentiment analysis model using the 'text-classification' pipeline\n",
        "# The model 'fpianz/roberta-english-book-reviews-sentiment' is used for classifying text into sentiments\n",
        "# The argument 'return_all_scores=True' ensures that the classifier returns scores for all sentiment classes\n",
        "classifier = pipeline(\"text-classification\", model=\"fpianz/roberta-english-book-reviews-sentiment\", return_all_scores=True)\n",
        "\n",
        "# Iterate over each CSV file in the list of CSV files\n",
        "for csv_file in csv_files:\n",
        "    # Construct the full file path for the current CSV file\n",
        "    file_path = os.path.join(folder_path, csv_file)\n",
        "\n",
        "    # Read the CSV file into a DataFrame\n",
        "    # This allows us to manipulate and analyze the data in the file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Add a new column 'positive_prob' to the DataFrame and initialize it with 0.0\n",
        "    # This column will store the probability of the review being positive\n",
        "    df['positive_prob'] = 0.0\n",
        "\n",
        "    # Iterate over each row in the DataFrame\n",
        "    # This allows us to analyze each review individually\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract the review text from the current row\n",
        "        review = row['cleaned_reviews']\n",
        "\n",
        "        # Get the sentiment analysis results for the review\n",
        "        # The classifier returns a list of dictionaries with labels and scores\n",
        "        results = classifier(review)\n",
        "\n",
        "        # Initialize a variable to store the positive probability\n",
        "        positive_prob = None\n",
        "        # Iterate over the results for the review\n",
        "        # Find the score corresponding to the 'positive' label\n",
        "        for result in results[0]:\n",
        "            if result['label'] == 'positive':\n",
        "                positive_prob = result['score']\n",
        "                break\n",
        "\n",
        "        # Update the 'positive_prob' column for the current row with the obtained positive probability\n",
        "        df.at[index, 'positive_prob'] = positive_prob\n",
        "\n",
        "    # Construct the full file path for the output CSV file\n",
        "    # This path is in the output folder and has the same name as the input CSV file\n",
        "    output_file_path = os.path.join(output_folder_path, csv_file)\n",
        "    df.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "YjWCCnN7a8zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Sentiment category statistics for book reviews"
      ],
      "metadata": {
        "id": "ibSjsvg6d0lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is to answer the subquestion: Are readers' book reviews about Science Fiction on the Goodreads website generally more positive or more negative?"
      ],
      "metadata": {
        "id": "jeYvzgAcr2wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of folder paths containing the filtered English-only review CSV files.\n",
        "folder_paths = [\n",
        "    \"/content/drive/MyDrive/page_1_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_2_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_3_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_4_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_5_reviews_english_only\"\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame to hold combined data from all CSV files.\n",
        "combined_data = pd.DataFrame()\n",
        "\n",
        "# Loop through each folder path in the list.\n",
        "for folder_path in folder_paths:\n",
        "    # Create a list of filenames for all CSV files in the current folder.\n",
        "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "\n",
        "    # Loop through each CSV file in the list of filenames.\n",
        "    for csv_file in csv_files:\n",
        "        # Construct the full file path of the current CSV file.\n",
        "        file_path = os.path.join(folder_path, csv_file)\n",
        "\n",
        "        # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "        data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "        # Concatenate the current DataFrame with the combined_data DataFrame, ignoring the index to avoid misalignment.\n",
        "        combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
        "\n",
        "# Count the occurrences of each unique value in the 'label' column of the combined DataFrame.\n",
        "label_counts = combined_data['label'].value_counts()\n",
        "\n",
        "# Calculate the percentage representation of each label.\n",
        "label_percentages = label_counts / label_counts.sum() * 100\n",
        "\n",
        "# Print the counts of each label.\n",
        "print(label_counts)\n",
        "# Print the percentage representation of each label.\n",
        "print(label_percentages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEj7pCCrd76M",
        "outputId": "f7b67f5d-6ac6-43b3-e6ac-bca3f5671f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "positive    19561\n",
            "negative    10127\n",
            "neutral      8078\n",
            "Name: count, dtype: int64\n",
            "label\n",
            "positive    51.795266\n",
            "negative    26.815125\n",
            "neutral     21.389610\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# List of folder paths containing the filtered English-only review CSV files.\n",
        "folder_paths = [\n",
        "    \"/content/drive/MyDrive/page_1_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_2_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_3_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_4_reviews_english_only\",\n",
        "    \"/content/drive/MyDrive/page_5_reviews_english_only\"\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame to hold combined data from all CSV files.\n",
        "combined_data = pd.DataFrame()\n",
        "\n",
        "# Loop through each folder path in the list.\n",
        "for folder_path in folder_paths:\n",
        "    # Create a list of filenames for all CSV files in the current folder.\n",
        "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "\n",
        "    # Loop through each CSV file in the list of filenames.\n",
        "    for csv_file in csv_files:\n",
        "        # Construct the full file path of the current CSV file.\n",
        "        file_path = os.path.join(folder_path, csv_file)\n",
        "\n",
        "        # Read the CSV file into a DataFrame, considering the first row as the header.\n",
        "        data = pd.read_csv(file_path, header=0)\n",
        "\n",
        "        # Concatenate the current DataFrame with the combined_data DataFrame, ignoring the index to avoid misalignment.\n",
        "        combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
        "\n",
        "# Create a pivot table from the combined DataFrame.\n",
        "# - `index='Rating'`: Rows will represent unique values in the 'Rating' column.\n",
        "# - `columns='label'`: Columns will represent unique values in the 'label' column.\n",
        "# - `aggfunc='size'`: Aggregation function to count the number of occurrences.\n",
        "# - `fill_value=0`: Replace any missing values with 0 in the pivot table.\n",
        "summary_table = combined_data.pivot_table(index='Rating', columns='label', aggfunc='size', fill_value=0)\n",
        "\n",
        "# Print the summary table to display the count of each label for each rating.\n",
        "print(summary_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgLd7QMpVefk",
        "outputId": "6987cccf-cf73-4a90-bfa1-4f7703f81997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label   negative  neutral  positive\n",
            "Rating                             \n",
            "1.0         1253      207        68\n",
            "2.0         2290      401       271\n",
            "3.0         2886     1550      2054\n",
            "4.0         2096     3260      7679\n",
            "5.0         1602     2660      9489\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}