{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping metadata and book reviews"
      ],
      "metadata": {
        "id": "mO_192AGHz26"
      },
      "id": "mO_192AGHz26"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this section is to scrape the specific information of all books in the list \"Can't Wait Sci-Fi/Fantasy of 2023\" on the Goodreads website (url: https://www.goodreads.com/list/show/171192). We need to get the information of all the books under this list and collect two types of data in total. The first is metadata, the specific data items include: the book title, author, average rating, total number of ratings, total number of reviews, genres, number of pages, and URL; The second is the book reviews, the specific data items include: username, time of the book review, content of the book review, user label, rating, and support of the book review (i.e., the number of likes)."
      ],
      "metadata": {
        "id": "c4rRSK6LH9WC"
      },
      "id": "c4rRSK6LH9WC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Installing the necessary libraries and Browser Driver"
      ],
      "metadata": {
        "id": "LPHecfEiMt0E"
      },
      "id": "LPHecfEiMt0E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the Goodreads website no longer provides an API, and it is impossible to access the full text of book reviews, we developed the following code. We use the web scraping libraries Beautiful Soup and Selenium to help collect data.\n",
        "\n",
        "Note: make sure you have installed the necessary libraries, such as Beautiful Soup and Selenium, before running the code. You can use pip, the Python package installer, to install these libraries.\n",
        "\n",
        "To install them, open your command prompt or terminal and enter the following commands:\n",
        "pip install selenium;\n",
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "sYIOCxO9M1T2"
      },
      "id": "sYIOCxO9M1T2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take ChromeDriver as an example, of course, you can also adopt other browser drivers such as Safari. Please follow the setup instruction https://sites.google.com/chromium.org/driver/getting-started"
      ],
      "metadata": {
        "id": "1fWMhwCAkm7e"
      },
      "id": "1fWMhwCAkm7e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from lxml import etree\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.alert import Alert\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "KBmxQUOXSw9f"
      },
      "id": "KBmxQUOXSw9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Analyzing the webpage structure"
      ],
      "metadata": {
        "id": "8PoXv50eTBJI"
      },
      "id": "8PoXv50eTBJI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step: browse the structure of target webpage. Based on the observation, we found the science fiction in the list \"Can't Wait Sci-Fi/Fantasy of 2023\" are presented in a table format. In the table, only partial information is shown, such as the title of science fiction and rating. The title of every science fiction corresponds to a hyperlink, and clicking on the hyperlink will turn to the detail page of that science fiction. The detail page contains the information we need, i.e. metadata and book reviews.\n",
        "\n",
        "In other words, we have to click on the hyperlink to get the specifical information. So, we set the following ideas:\n",
        "(1) Obtain a complete link list corresponding to the science fiction in the list \"Can't Wait Sci-Fi/Fantasy of 2023\";\n",
        "(2) Iterate through the target list of links to scrape the specifical information from every novel webpage;\n",
        "(3) Store the data locally."
      ],
      "metadata": {
        "id": "aQR4PKAOTOkD"
      },
      "id": "aQR4PKAOTOkD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Obtaining the URL list"
      ],
      "metadata": {
        "id": "WvWPF4y92guw"
      },
      "id": "WvWPF4y92guw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that the URL is contained the \"href\" attribute of novel’s title with the help of developer tool. Selenium provides several ways to locate the elements, like XPath, ID and name. In this case, we choose to use XPath to locate the title element of every novel.\n",
        "\n",
        "The list “Can't Wait Sci-Fi/Fantasy of 2023” is divided into 5 pages, and its url has a certain pattern.\n",
        "\n",
        "The URL for page 1 is:\n",
        "https://www.goodreads.com/list/show/171192.Can_t_Wait_Sci_Fi_Fantasy_of_2023?page=1\n",
        "\n",
        "The URL for page 2 is:\n",
        "https://www.goodreads.com/list/show/171192.Can_t_Wait_Sci_Fi_Fantasy_of_2023?page=2\n",
        "\n",
        "The URL for page 3 is:\n",
        "https://www.goodreads.com/list/show/171192.Can_t_Wait_Sci_Fi_Fantasy_of_2023?page=3\n",
        "\n",
        "The URL for page 4 is:\n",
        "https://www.goodreads.com/list/show/171192.Can_t_Wait_Sci_Fi_Fantasy_of_2023?page=4\n",
        "\n",
        "The URL for page 5 is:\n",
        "https://www.goodreads.com/list/show/171192.Can_t_Wait_Sci_Fi_Fantasy_of_2023?page=5\n",
        "\n",
        "So, the code can be reused by slightly adjusting the URL for each page.\n"
      ],
      "metadata": {
        "id": "SzAqJG2yDAEa"
      },
      "id": "SzAqJG2yDAEa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64759d97",
      "metadata": {
        "id": "64759d97"
      },
      "outputs": [],
      "source": [
        "# Define the path to the ChromeDriver executable.\n",
        "chromedriver_path = '/Users/wanshuo/Desktop/Master/DH_MA_thesis/dataset/chromedriver-mac-arm64/chromedriver'\n",
        "\n",
        "# Create an instance of ChromeOptions to specify additional options for the Chrome browser.\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "\n",
        "# Add an argument to Chrome options to specify the path to the ChromeDriver executable.\n",
        "chrome_options.add_argument(f'--webdriver-path={chromedriver_path}')\n",
        "\n",
        "# Initialize the Chrome WebDriver with the specified options.\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Navigate to the specified URL (a list page for Sci-Fi/Fantasy books of 2023).\n",
        "driver.get('https://www.goodreads.com/list/show/171192.Can_t_Wait_Sci_Fi_Fantasy_of_2023?page=1')\n",
        "\n",
        "# Pause the execution for 10 seconds to allow the page to fully load.\n",
        "time.sleep(10)\n",
        "\n",
        "# Simulate pressing the PAGE_DOWN key 60 times to scroll down the webpage.\n",
        "for i in range(60):\n",
        "    # Simulate the Page Down key press using ActionChains\n",
        "    ActionChains(driver).key_down(Keys.PAGE_DOWN).key_up(Keys.PAGE_DOWN).perform()\n",
        "    # Pause the execution for 0.2 seconds between each scroll to mimic natural user behavior.\n",
        "    time.sleep(0.2)\n",
        "\n",
        "# Initialize an empty list to store all URLs extracted from the webpage.\n",
        "all_url_list = []\n",
        "\n",
        "# Because the URL is contained within the \"href\" attribute of the novel's title，\n",
        "# define the XPath expression to locate the URLs.\n",
        "url_xpath = '//*[@id=\"all_votes\"]/table/tbody/tr/td[3]/a/@href'\n",
        "\n",
        "# Get the page source (HTML content) of the current webpage.\n",
        "html = driver.page_source\n",
        "# Parse the HTML content using lxml's etree to create an HTML tree.\n",
        "tree = etree.HTML(html)\n",
        "# Extract the URLs from the HTML tree using the specified XPath expression.\n",
        "url_list = tree.xpath(url_xpath)\n",
        "# Add the extracted URLs to the list of all URLs.\n",
        "all_url_list.extend(url_list)\n",
        "\n",
        "# Close the WebDriver and quit the browser.\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Iterating through the URL list to scrape specifical information"
      ],
      "metadata": {
        "id": "tVCu0kng2lz4"
      },
      "id": "tVCu0kng2lz4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 getting metadata\n",
        "Selenium is a powerful tool for automating web browsing, it can simulate various actions human beings take on a webpage, like clicking on the button and scrolling webpages. We can scrape the data we need easily without being detected by the website in this way (Selenium, 2023). In this programme, we utilize Selenium to simulate the human action of browsing a webpage so as to scrape metadata, i.e., the book title, author, average rating, total number of ratings, total number of reviews, genres, number of pages."
      ],
      "metadata": {
        "id": "fDxnVk_qETCE"
      },
      "id": "fDxnVk_qETCE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be054d5",
      "metadata": {
        "id": "0be054d5",
        "outputId": "31701c21-5999-4b66-e66d-f96b62f39368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Book 1: Witch King - Martha Wells, Rating: 3.72, Rating Count: 15,068, Text Reviews Count: 3,264, Genres: Fantasy;Fiction;Adult;Magic;High Fantasy;Witches;Science Fiction, Num Pages: 424\n",
            "Book 2: System Collapse - Martha Wells, Rating: 4.23, Rating Count: 33,540, Text Reviews Count: 4,188, Genres: Science Fiction;Fiction;Audiobook;Adult;Science Fiction Fantasy;Space;Space Opera, Num Pages: 245\n",
            "Book 3: Hell Bent - Leigh Bardugo, Rating: 4.15, Rating Count: 114,636, Text Reviews Count: 15,911, Genres: Fantasy;Fiction;Horror;Mystery;Urban Fantasy;Paranormal;Adult, Num Pages: 484\n",
            "Book 4: Tress of the Emerald Sea - Brandon Sanderson, Rating: 4.43, Rating Count: 113,398, Text Reviews Count: 17,504, Genres: Fantasy;Fiction;Romance;Adventure;Young Adult;High Fantasy;Audiobook, Num Pages: 483\n",
            "Book 5: A Day of Fallen Night - Samantha    Shannon, Rating: 4.39, Rating Count: 28,528, Text Reviews Count: 5,008, Genres: Fantasy;Fiction;LGBT;Queer;Adult;Lesbian;Dragons, Num Pages: 868\n",
            "Book 6: The Adventures of Amina al-Sirafi - Shannon Chakraborty, Rating: 4.30, Rating Count: 39,313, Text Reviews Count: 7,941, Genres: Fantasy;Historical Fiction;Fiction;Pirates;Adult;Adventure;Historical, Num Pages: 496\n",
            "Book 7: Untethered Sky - Fonda Lee, Rating: 3.90, Rating Count: 7,476, Text Reviews Count: 1,763, Genres: Fantasy;Novella;Fiction;Adult;Audiobook;High Fantasy;Short Stories, Num Pages: 152\n",
            "Book 8: The Cheat Code - Misba, Rating: 4.57, Rating Count: 2,512, Text Reviews Count: 893, Genres: Fantasy;Science Fiction Fantasy;Science Fiction;Post Apocalyptic;Adventure;Adult;Thriller, Num Pages: 465\n",
            "Book 9: The Stolen Heir - Holly Black, Rating: 4.00, Rating Count: 96,384, Text Reviews Count: 17,343, Genres: Fantasy;Young Adult;Romance;Fae;Young Adult Fantasy;Fiction;Magic, Num Pages: 11\n",
            "Book 10: Yumi and the Nightmare Painter - Brandon Sanderson, Rating: 4.49, Rating Count: 45,875, Text Reviews Count: 6,717, Genres: Fantasy;Romance;Fiction;Adult;High Fantasy;Science Fiction;Audiobook, Num Pages: 480\n",
            "Book 11: A House with Good Bones - T. Kingfisher, Rating: 3.70, Rating Count: 32,404, Text Reviews Count: 5,867, Genres: Horror;Fantasy;Fiction;Gothic;Mystery;Paranormal;Adult, Num Pages: 247\n",
            "Book 12: The Sunlit Man - Brandon Sanderson, Rating: 4.37, Rating Count: 27,909, Text Reviews Count: 3,095, Genres: Fantasy;Fiction;Science Fiction;Epic Fantasy;Adult;Audiobook;High Fantasy, Num Pages: 447\n",
            "Book 13: Light Bringer - Pierce Brown, Rating: 4.75, Rating Count: 34,653, Text Reviews Count: 3,770, Genres: Science Fiction;Fantasy;Fiction;Dystopia;Audiobook;Space Opera;Adult, Num Pages: 682\n",
            "Book 14: The Remarkable Retirement of Edna Fisher - E.M.   Anderson, Rating: 3.75, Rating Count: 903, Text Reviews Count: 199, Genres: Fantasy;Fiction;Dragons;Adult;LGBT;Queer;Cozy Mystery, Num Pages: 370\n",
            "Book 15: The Frugal Wizard’s Handbook for Surviving Medieval England - Brandon Sanderson, Rating: 3.77, Rating Count: 39,183, Text Reviews Count: 5,204, Genres: Fantasy;Science Fiction;Fiction;Audiobook;Adult;Time Travel;Humor, Num Pages: 399\n",
            "Book 16: Heavenly Tyrant - Xiran Jay Zhao, Rating: 4.30, Rating Count: 191, Text Reviews Count: 75, Genres: Fantasy;Science Fiction;Young Adult;Queer;Romance;LGBT;Fiction, Num Pages: 400\n",
            "Book 17: He Who Drowned the World - Shelley Parker-Chan, Rating: 4.29, Rating Count: 6,662, Text Reviews Count: 1,639, Genres: Fantasy;Historical Fiction;Queer;Historical;Fiction;LGBT;Adult, Num Pages: 486\n",
            "Book 18: Lost in the Moment and Found - Seanan McGuire, Rating: 4.33, Rating Count: 12,542, Text Reviews Count: 2,351, Genres: Fantasy;Young Adult;Novella;Audiobook;Fiction;Magical Realism;Adult, Num Pages: 161\n",
            "Book 19: Some Desperate Glory - Emily Tesh, Rating: 4.06, Rating Count: 7,037, Text Reviews Count: 1,647, Genres: Science Fiction;LGBT;Queer;Fiction;Fantasy;Adult;Lesbian, Num Pages: 438\n",
            "Book 20: In the Lives of Puppets - T.J. Klune, Rating: 3.94, Rating Count: 58,593, Text Reviews Count: 11,021, Genres: Fantasy;Science Fiction;Fiction;LGBT;Romance;Queer;Adult, Num Pages: 432\n",
            "Book 21: Kill Your Darlings - L.E. Harper, Rating: 4.16, Rating Count: 269, Text Reviews Count: 196, Genres: Fantasy;Dragons;LGBT;Mental Health;Magic;Asexual;Adult, Num Pages: 320\n",
            "Book 22: The Deep Sky - Yume Kitasei, Rating: 3.71, Rating Count: 5,834, Text Reviews Count: 1,207, Genres: Science Fiction;Mystery;Fiction;Thriller;Adult;Mystery Thriller;Space, Num Pages: 399\n",
            "Book 23: Emily Wilde's Encyclopaedia of Faeries - Heather Fawcett, Rating: 4.05, Rating Count: 78,891, Text Reviews Count: 15,596, Genres: Fantasy;Romance;Fiction;Historical Fiction;Adult;Fae;Historical, Num Pages: 317\n",
            "Book 24: Translation State - Ann Leckie, Rating: 4.13, Rating Count: 8,656, Text Reviews Count: 1,225, Genres: Science Fiction;Fiction;Space Opera;Queer;LGBT;Fantasy;Science Fiction Fantasy, Num Pages: 13\n",
            "No number of pages found for URL: https://www.goodreads.com/book/show/59992791-the-line-that-cannot-be-crossed.\n",
            "Book 25: The Line That Cannot Be Crossed - Ian Gregoire, Rating: 5.00, Rating Count: 1, Text Reviews Count: 1, Genres: , Num Pages: 13\n",
            "Book 26: Godkiller - Hannah Kaner, Rating: 3.93, Rating Count: 24,677, Text Reviews Count: 5,211, Genres: Fantasy;Fiction;Adult;Mythology;LGBT;Queer;High Fantasy, Num Pages: 304\n"
          ]
        }
      ],
      "source": [
        "# Path to the ChromeDriver executable\n",
        "chromedriver_path = '/Users/wanshuo/Desktop/Master/DH_MA_thesis/dataset/chromedriver-mac-arm64/chromedriver'\n",
        "\n",
        "# Create a ChromeOptions object to configure ChromeDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "\n",
        "# Disable image loading to speed up page load times\n",
        "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
        "# Add the image display preference to the Chrome options\n",
        "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "# Add the path to ChromeDriver as an argument\n",
        "chrome_options.add_argument(f'--webdriver-path={chromedriver_path}')\n",
        "\n",
        "# Create a new instance of Chrome WebDriver with the specified options\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Define the XPaths for the elements we want to scrape\n",
        "title_xpath = '//*[@id=\"__next\"]/div[2]/main/div[1]/div[2]/div[2]/div[1]/div[1]/h1'\n",
        "author_xpath = '//*[@id=\"__next\"]/div[2]/main/div[1]/div[2]/div[2]/div[2]/div[1]/h3/div/span[1]/a/span[1]'\n",
        "average_rating_xpath = '//*[@id=\"__next\"]/div[2]/main/div[1]/div[2]/div[2]/div[2]/div[2]/a/div[1]/div'\n",
        "rating_count_xpath = '//*[@id=\"__next\"]/div[2]/main/div[1]/div[2]/div[2]/div[2]/div[2]/a/div[2]/div/span[1]'\n",
        "text_reviews_rating_count_xpath = '//*[@id=\"__next\"]/div[2]/main/div[1]/div[2]/div[2]/div[2]/div[2]/a/div[2]/div/span[2]'\n",
        "\n",
        "# Create an empty list to store all the book title, author, average rating, total number of ratings, total number of reviews, genres, number of pages.\n",
        "all_title = []\n",
        "all_author = []\n",
        "all_average_rating = []\n",
        "all_rating_count = []\n",
        "all_text_reviews_rating_count = []\n",
        "all_genres = []\n",
        "all_num_pages = []\n",
        "\n",
        "# List to store URLs that were successfully scraped\n",
        "successful_urls = []\n",
        "\n",
        "# Iterate over each URL in the list of URLs to scrape\n",
        "for url_item in all_url_list:\n",
        "    url = 'https://www.goodreads.com' + url_item  # Construct the full URL\n",
        "    driver.get(url)  # Open the URL in the browser\n",
        "    time.sleep(5)  # Wait for the page to load completely\n",
        "\n",
        "    # Maximize the browser window to ensure all elements are visible\n",
        "    driver.maximize_window()\n",
        "\n",
        "    # XPath for the close button of any pop-up that might appear\n",
        "    close_button_xpath = '/html/body/div[3]/div/div[1]/div'\n",
        "    try:\n",
        "        # Try to find and click the close button if it exists\n",
        "        close_button = driver.find_element(By.XPATH, close_button_xpath)\n",
        "        close_button.click()\n",
        "    except NoSuchElementException:\n",
        "        pass  # If the close button is not found, proceed without error\n",
        "\n",
        "    # Wait for any dynamic content to load\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Get the page source (HTML) of the current page\n",
        "    html = driver.page_source\n",
        "    # Parse the HTML with lxml\n",
        "    tree = etree.HTML(html)\n",
        "\n",
        "    # Check if the page contains a \"Page not found\" message\n",
        "    if 'Sorry, the page you requested could not be found.' in driver.page_source:\n",
        "        print(f\"Page not found for URL: {url}. Skipping to the next book.\")\n",
        "        continue\n",
        "\n",
        "    # If the page is found, add the URL to the list of successful URLs\n",
        "    successful_urls.append(url)\n",
        "\n",
        "    # Extract and append the book title from the page\n",
        "    title_list = tree.xpath(title_xpath + '/text()')\n",
        "    all_title.append(title_list[0])\n",
        "\n",
        "    # Extract and append the author from the page\n",
        "    author_list = tree.xpath(author_xpath + '/text()')\n",
        "    all_author.append(author_list[0])\n",
        "\n",
        "    # Extract and append the average rating from the page\n",
        "    average_rating_list = tree.xpath(average_rating_xpath + '/text()')\n",
        "    all_average_rating.append(average_rating_list[0])\n",
        "\n",
        "    # Extract and append the rating count from the page\n",
        "    rating_count_list = tree.xpath(rating_count_xpath + '/text()')\n",
        "    all_rating_count.append(rating_count_list[0])\n",
        "\n",
        "    # Extract and append the text reviews count from the page\n",
        "    text_reviews_rating_count_list = tree.xpath(text_reviews_rating_count_xpath + '/text()')\n",
        "    all_text_reviews_rating_count.append(text_reviews_rating_count_list[0])\n",
        "\n",
        "    # Attempt to find and extract the genres for the book\n",
        "    try:\n",
        "        # Find all elements that match the genre button's XPath\n",
        "        genres_elements = driver.find_elements(By.XPATH, '//span[contains(@class, \"BookPageMetadataSection__genreButton\")]/a[@class=\"Button Button--tag-inline Button--small\"]/span[@class=\"Button__labelItem\"]')\n",
        "        # Extract the text from each genre element and strip any surrounding whitespace\n",
        "        genres_list = [element.text.strip() for element in genres_elements]\n",
        "        # Join the genres into a single string separated by semicolons, as there may be multiple genres\n",
        "        content = ';'.join(genres_list)\n",
        "        # Append the joined genres string to the all_genres list\n",
        "        all_genres.append(content)\n",
        "    # Handle the case where the genre elements are not found\n",
        "    except NoSuchElementException:\n",
        "        # Print a message indicating no genres were found for the URL\n",
        "        print(f\"No genres found for URL: {url}.\")\n",
        "        # Append \"N/A\" to the all_genres list to indicate that genres are not available\n",
        "        all_genres.append(\"N/A\")\n",
        "\n",
        "    # Attempt to find and extract the number of pages for the book\n",
        "    try:\n",
        "        # Find all elements that match the number of pages format's XPath\n",
        "        num_pages_elements = driver.find_elements(By.XPATH, '//p[@data-testid=\"pagesFormat\"]')\n",
        "        # Extract the number from each element's text and compile into a list\n",
        "        num_pages_list = [re.search(r'\\d+', element.text.strip()).group() for element in num_pages_elements]\n",
        "        # Append the first found number of pages to the all_num_pages list\n",
        "        all_num_pages.append(num_pages_list[0])\n",
        "    # Handle the case where the number of pages elements are not found or text extraction fails\n",
        "    except (NoSuchElementException, IndexError, AttributeError):\n",
        "        # Print a message indicating no number of pages were found for the URL\n",
        "        print(f\"No number of pages found for URL: {url}.\")\n",
        "        # Append \"N/A\" to the all_num_pages list to indicate that the number of pages is not available\n",
        "        all_num_pages.append(\"N/A\")\n",
        "\n",
        "    # Print the scraped details for the current book\n",
        "    print(f\"Book {index + 1}: {title_list[0]} - {author_list[0]}, Rating: {average_rating_list[0]}, Rating Count: {rating_count_list[0]}, Text Reviews Count: {text_reviews_rating_count_list[0]}, Genres: {content}, Num Pages: {num_pages_list[0]}\")\n",
        "\n",
        "# Close the browser window\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Storing metadata locally"
      ],
      "metadata": {
        "id": "uIVMOxVS3Wyt"
      },
      "id": "uIVMOxVS3Wyt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84e60a6",
      "metadata": {
        "id": "f84e60a6"
      },
      "outputs": [],
      "source": [
        "# Define the path to the CSV file\n",
        "csv_file_path = 'Page_1_data_list.csv'\n",
        "\n",
        "# Open the CSV file for writing\n",
        "# 'w' mode means write; if the file does not exist, it will be created\n",
        "# encoding='utf-8' ensures that the file is encoded in UTF-8 to handle special characters\n",
        "# newline='' prevents extra blank lines in the CSV file\n",
        "with open(csv_file_path, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "\n",
        "    # Create a CSV writer object\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row to the CSV file\n",
        "    writer.writerow(['title', 'author', 'average_rating', 'num_pages', 'rating_count', 'text_reviews_rating_count', 'genres', 'url'])\n",
        "\n",
        "    # Iterate over the combined data lists using the zip function\n",
        "    # Each iteration returns a tuple containing one item from each list\n",
        "    # This is used to write a row of data for each book\n",
        "    for url_item, title_item, author_item, average_rating_item, num_pages_item, rating_count_item, text_reviews_rating_count_item, genres_item in zip(successful_urls, all_title, all_author, all_average_rating, all_num_pages, all_rating_count, all_text_reviews_rating_count, all_genres):\n",
        "\n",
        "        # Write a row of data to the CSV file\n",
        "        # The order of items in the list matches the header row\n",
        "        writer.writerow([title_item, author_item, average_rating_item, num_pages_item, rating_count_item, text_reviews_rating_count_item, genres_item, url_item])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference**\n",
        "\n",
        "Selenium. (2023, June 27). https://www.selenium.dev/"
      ],
      "metadata": {
        "id": "4vVCXzpfKZEn"
      },
      "id": "4vVCXzpfKZEn"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}